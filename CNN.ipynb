{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "# used to access folder structures\n",
    "import os\n",
    "\n",
    "# used to open images\n",
    "import PIL\n",
    "\n",
    "# Graphs, visualizations\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# For Image Data Augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data From Pickle file\n",
    "\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    y_data = pickle.load(f)\n",
    "f.close()\n",
    "with open('X.pickle', 'rb') as f:\n",
    "    X_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166514, 50, 50, 3) (166514,) 0.7144444310988866\n",
      "(55505, 50, 50, 3) (55505,) 0.7182415998558689\n",
      "(55505, 50, 50, 3) (55505,) 0.7189802720475633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train-validation-test split\n",
    "\n",
    "# train test split for validation after training x_test is never touched or looked at during training\n",
    "x_train,x_test,y_train,y_test=sklearn.model_selection.train_test_split(np.asarray(X_data),\n",
    "                                                                       np.asarray(y_data),\n",
    "                                                                       test_size=.2,\n",
    "                                                                       random_state=42)\n",
    "\n",
    "# train test split for validation during training\n",
    "x_train,x_val,y_train,y_val=sklearn.model_selection.train_test_split(x_train,\n",
    "                                                                     y_train,\n",
    "                                                                     test_size=.25,\n",
    "                                                                     random_state=42)\n",
    "\n",
    "#Dimension of the kaggle dataset & percentage of negative patches\n",
    "print(x_train.shape,y_train.shape, 1-sum(y_train)/y_train.shape[0])\n",
    "print(x_test.shape,y_test.shape, 1-sum(y_test)/y_test.shape[0])\n",
    "print(x_val.shape,y_val.shape, 1-sum(y_val)/y_val.shape[0])\n",
    "\n",
    "input_shape=x_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_plot(history):\n",
    "    plt.plot(history.history['accuracy'], alpha=.6)\n",
    "    plt.plot(history.history['val_accuracy'], alpha=.6)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "    return plt\n",
    "\n",
    "def loss_plot(history):\n",
    "    plt.plot(history.history['loss'][1:], alpha=.6)\n",
    "    plt.plot(history.history['val_loss'], alpha=.6)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "    return plt\n",
    "def plot_prc(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "def conf_matrix(model, x_test, y_test):\n",
    "    \n",
    "    y_pred = [1 * (x[0]>=0.5) for x in model.predict(x_test)]\n",
    "\n",
    "    matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    df_cm = pandas.DataFrame(matrix, index = [i for i in ['No Cancer (actual)', 'Cancer (actual)']],\n",
    "                      columns = [i for i in ['predict No Cancer', 'predict Cancer']])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True, fmt='d')\n",
    "    return plt\n",
    "\n",
    "def acc_df(histories):\n",
    "    columns = ['model size' ,'loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    for history in histories:\n",
    "    \n",
    "    # get the epoch with the highest validation accuracy for each history element\n",
    "        i = 0\n",
    "        index = 0\n",
    "        comp = 0\n",
    "        for val_acc in history.history['val_accuracy']:\n",
    "            if val_acc > comp:\n",
    "                comp = val_acc\n",
    "                i = index\n",
    "            index += 1\n",
    "\n",
    "        df_temp = pd.DataFrame([[history.model.name,\n",
    "                                  history.history['loss'][i],\n",
    "                                  history.history['accuracy'][i],\n",
    "                                  history.history['val_loss'][i],\n",
    "                                  history.history['val_accuracy'][i]]], \n",
    "                        columns = columns)\n",
    "        df = df.append(df_temp)\n",
    "\n",
    "    df = df.set_index('model size')\n",
    "\n",
    "    df = df.style.format({\n",
    "        'loss': '{:,.2f}'.format,\n",
    "        'accuracy': '{:,.2%}'.format,\n",
    "        'val_loss': '{:,.2f}'.format,\n",
    "        'val_accuracy': '{:,.2%}'.format,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_accuracies(histories, lower_ylim=.7, upper_ylim=.9, alpha=.6, **kwargs):\n",
    "    for history in histories:\n",
    "        plt.plot(history.history['accuracy'], alpha=alpha)\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([lower_ylim,upper_ylim])\n",
    "    plt.legend([hist.model.name for hist in histories], loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    for history in histories:\n",
    "        plt.plot(history.history['val_accuracy'], alpha=alpha)\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([lower_ylim,upper_ylim])\n",
    "    plt.legend([hist.model.name for hist in histories], loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "### Callbacks ###\n",
    "\n",
    "#Early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, verbose=2,\n",
    "                                      mode='max', baseline=None, restore_best_weights=True)\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "#Learning Rate Annealer\n",
    "lrr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                       factor=.1,\n",
    "                       patience=20,\n",
    "                       min_lr=1e-4,\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 1.40\n",
      "Weight for class 1: 3.52\n"
     ]
    }
   ],
   "source": [
    "# lower the number of epochs\n",
    "epochs = 150\n",
    "\n",
    "total = np.asarray(y_data).shape[0]\n",
    "pos = sum(np.asarray(y_data))\n",
    "neg = total - pos\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "# weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "weight_for_0 = (1 / neg) * total\n",
    "weight_for_1 = (1 / pos) * total\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simple Convolutional Network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> From the dense network notebook we learned, that the learning rate of 0.001 was a good learning rate to take for the simple cnn. The simple convolutional networks just use simple dense networks</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "histories1 = []\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "651/651 [==============================] - 31s 44ms/step - loss: 1.0922 - accuracy: 0.7898 - val_loss: 1.0940 - val_accuracy: 0.7261\n",
      "Epoch 2/100\n",
      "651/651 [==============================] - 28s 43ms/step - loss: 0.7983 - accuracy: 0.8265 - val_loss: 1.0575 - val_accuracy: 0.5072\n",
      "Epoch 3/100\n",
      "651/651 [==============================] - 28s 43ms/step - loss: 0.7710 - accuracy: 0.8302 - val_loss: 1.8677 - val_accuracy: 0.7624\n",
      "Epoch 4/100\n",
      "651/651 [==============================] - 28s 43ms/step - loss: 0.7607 - accuracy: 0.8330 - val_loss: 0.6324 - val_accuracy: 0.7800\n",
      "Epoch 5/100\n",
      "651/651 [==============================] - 28s 43ms/step - loss: 0.7553 - accuracy: 0.8342 - val_loss: 0.4761 - val_accuracy: 0.7801\n",
      "Epoch 6/100\n",
      "381/651 [================>.............] - ETA: 10s - loss: 0.7350 - accuracy: 0.8389"
     ]
    }
   ],
   "source": [
    "name=\"simple-cnn-1\"\n",
    "model1 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(64, kernel_size = (5, 5), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    ],name=name\n",
    ")\n",
    "\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "histories1.append(\n",
    "    model1.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs, \n",
    "        validation_data=(x_val,y_val), \n",
    "        callbacks=[lrr, es], \n",
    "        class_weight=class_weight))\n",
    "\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "name=\"simple-cnn-2\"\n",
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(64, kernel_size = (5, 5), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, kernel_size = (5, 5), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    ],name=name\n",
    ")\n",
    "\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "histories1.append(\n",
    "    model2.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs, \n",
    "        validation_data=(x_val,y_val), \n",
    "        callbacks=[lrr, es], \n",
    "        class_weight=class_weight))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "name=\"simple-cnn-3\"\n",
    "model3 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        \n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    ],name=name\n",
    ")\n",
    "\n",
    "model3.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "histories1.append(\n",
    "    model3.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs, \n",
    "        validation_data=(x_val,y_val), \n",
    "        callbacks=[lrr, es], \n",
    "        class_weight=class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df(histories1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(histories1, lower_ylim=0, upper_ylim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Learnings </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Testtesttest </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Advanced CNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adv-cnn1-1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 723,137\n",
      "Trainable params: 714,433\n",
      "Non-trainable params: 8,704\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2602/2602 [==============================] - 129s 49ms/step - loss: 0.6042 - accuracy: 0.7256 - val_loss: 0.4224 - val_accuracy: 0.8181\n",
      "Epoch 2/100\n",
      "2602/2602 [==============================] - 132s 51ms/step - loss: 0.4726 - accuracy: 0.8015 - val_loss: 0.4009 - val_accuracy: 0.8261\n",
      "Epoch 3/100\n",
      "2602/2602 [==============================] - 134s 51ms/step - loss: 0.4489 - accuracy: 0.8089 - val_loss: 0.3893 - val_accuracy: 0.8330\n",
      "Epoch 4/100\n",
      "2602/2602 [==============================] - 134s 52ms/step - loss: 0.4347 - accuracy: 0.8155 - val_loss: 0.3829 - val_accuracy: 0.8335\n",
      "Epoch 5/100\n",
      "2602/2602 [==============================] - 135s 52ms/step - loss: 0.4201 - accuracy: 0.8219 - val_loss: 0.3735 - val_accuracy: 0.8387\n",
      "Epoch 6/100\n",
      "2602/2602 [==============================] - 139s 53ms/step - loss: 0.4132 - accuracy: 0.8245 - val_loss: 0.3686 - val_accuracy: 0.8408\n",
      "Epoch 7/100\n",
      "2602/2602 [==============================] - 162s 62ms/step - loss: 0.4049 - accuracy: 0.8291 - val_loss: 0.3678 - val_accuracy: 0.8395\n",
      "Epoch 8/100\n",
      "2602/2602 [==============================] - 176s 68ms/step - loss: 0.3957 - accuracy: 0.8320 - val_loss: 0.3621 - val_accuracy: 0.8427\n",
      "Epoch 9/100\n",
      "2602/2602 [==============================] - 156s 60ms/step - loss: 0.3929 - accuracy: 0.8345 - val_loss: 0.3668 - val_accuracy: 0.8384\n",
      "Epoch 10/100\n",
      "2602/2602 [==============================] - 148s 57ms/step - loss: 0.3839 - accuracy: 0.8370 - val_loss: 0.3590 - val_accuracy: 0.8481\n",
      "Epoch 11/100\n",
      "2602/2602 [==============================] - 137s 53ms/step - loss: 0.3835 - accuracy: 0.8364 - val_loss: 0.3545 - val_accuracy: 0.8480\n",
      "Epoch 12/100\n",
      "2602/2602 [==============================] - 151s 58ms/step - loss: 0.3819 - accuracy: 0.8387 - val_loss: 0.3664 - val_accuracy: 0.8448\n",
      "Epoch 13/100\n",
      "2602/2602 [==============================] - 143s 55ms/step - loss: 0.3769 - accuracy: 0.8407 - val_loss: 0.3643 - val_accuracy: 0.8462\n",
      "Epoch 14/100\n",
      "2602/2602 [==============================] - 135s 52ms/step - loss: 0.3735 - accuracy: 0.8421 - val_loss: 0.3563 - val_accuracy: 0.8492\n",
      "Epoch 15/100\n",
      "2602/2602 [==============================] - 132s 51ms/step - loss: 0.3725 - accuracy: 0.8418 - val_loss: 0.3465 - val_accuracy: 0.8520\n",
      "Epoch 16/100\n",
      "2602/2602 [==============================] - 184s 71ms/step - loss: 0.3733 - accuracy: 0.8407 - val_loss: 0.3451 - val_accuracy: 0.8526\n",
      "Epoch 17/100\n",
      "2602/2602 [==============================] - 214s 82ms/step - loss: 0.3640 - accuracy: 0.8459 - val_loss: 0.3447 - val_accuracy: 0.8519\n",
      "Epoch 18/100\n",
      "2602/2602 [==============================] - 213s 82ms/step - loss: 0.3653 - accuracy: 0.8439 - val_loss: 0.3454 - val_accuracy: 0.8529\n",
      "Epoch 19/100\n",
      "2602/2602 [==============================] - 217s 83ms/step - loss: 0.3614 - accuracy: 0.8461 - val_loss: 0.3444 - val_accuracy: 0.8525\n",
      "Epoch 20/100\n",
      "2602/2602 [==============================] - 213s 82ms/step - loss: 0.3640 - accuracy: 0.8436 - val_loss: 0.3430 - val_accuracy: 0.8525\n",
      "Epoch 21/100\n",
      "2602/2602 [==============================] - 215s 83ms/step - loss: 0.3617 - accuracy: 0.8463 - val_loss: 0.3428 - val_accuracy: 0.8534\n",
      "Epoch 22/100\n",
      "2602/2602 [==============================] - 214s 82ms/step - loss: 0.3604 - accuracy: 0.8461 - val_loss: 0.3394 - val_accuracy: 0.8545\n",
      "Epoch 23/100\n",
      "2602/2602 [==============================] - 126s 48ms/step - loss: 0.3547 - accuracy: 0.8497 - val_loss: 0.3404 - val_accuracy: 0.8546\n",
      "Epoch 24/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3534 - accuracy: 0.8502 - val_loss: 0.3415 - val_accuracy: 0.8535\n",
      "Epoch 25/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3521 - accuracy: 0.8511 - val_loss: 0.3392 - val_accuracy: 0.8547\n",
      "Epoch 26/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3535 - accuracy: 0.8489 - val_loss: 0.3412 - val_accuracy: 0.8541\n",
      "Epoch 27/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3524 - accuracy: 0.8507 - val_loss: 0.3380 - val_accuracy: 0.8556\n",
      "Epoch 28/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3517 - accuracy: 0.8520 - val_loss: 0.3462 - val_accuracy: 0.8514\n",
      "Epoch 29/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3520 - accuracy: 0.8503 - val_loss: 0.3379 - val_accuracy: 0.8553\n",
      "Epoch 30/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3512 - accuracy: 0.8508 - val_loss: 0.3470 - val_accuracy: 0.8518\n",
      "Epoch 31/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3497 - accuracy: 0.8509 - val_loss: 0.3347 - val_accuracy: 0.8570\n",
      "Epoch 32/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3476 - accuracy: 0.8513 - val_loss: 0.3344 - val_accuracy: 0.8573\n",
      "Epoch 33/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3487 - accuracy: 0.8508 - val_loss: 0.3392 - val_accuracy: 0.8557\n",
      "Epoch 34/100\n",
      "2602/2602 [==============================] - 120s 46ms/step - loss: 0.3452 - accuracy: 0.8527 - val_loss: 0.3314 - val_accuracy: 0.8586\n",
      "Epoch 35/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3432 - accuracy: 0.8541 - val_loss: 0.3322 - val_accuracy: 0.8577\n",
      "Epoch 36/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3440 - accuracy: 0.8538 - val_loss: 0.3298 - val_accuracy: 0.8596\n",
      "Epoch 37/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3466 - accuracy: 0.8526 - val_loss: 0.3303 - val_accuracy: 0.8596\n",
      "Epoch 38/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3396 - accuracy: 0.8554 - val_loss: 0.3307 - val_accuracy: 0.8588\n",
      "Epoch 39/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3413 - accuracy: 0.8556 - val_loss: 0.3327 - val_accuracy: 0.8585\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3400 - accuracy: 0.8560 - val_loss: 0.3275 - val_accuracy: 0.8606\n",
      "Epoch 41/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3387 - accuracy: 0.8555 - val_loss: 0.3291 - val_accuracy: 0.8601\n",
      "Epoch 42/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3379 - accuracy: 0.8555 - val_loss: 0.3267 - val_accuracy: 0.8604\n",
      "Epoch 43/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3364 - accuracy: 0.8570 - val_loss: 0.3319 - val_accuracy: 0.8583\n",
      "Epoch 44/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3367 - accuracy: 0.8564 - val_loss: 0.3312 - val_accuracy: 0.8584\n",
      "Epoch 45/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3350 - accuracy: 0.8580 - val_loss: 0.3258 - val_accuracy: 0.8614\n",
      "Epoch 46/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3333 - accuracy: 0.8589 - val_loss: 0.3257 - val_accuracy: 0.8614\n",
      "Epoch 47/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3386 - accuracy: 0.8563 - val_loss: 0.3225 - val_accuracy: 0.8629\n",
      "Epoch 48/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3315 - accuracy: 0.8579 - val_loss: 0.3261 - val_accuracy: 0.8600\n",
      "Epoch 49/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3322 - accuracy: 0.8584 - val_loss: 0.3264 - val_accuracy: 0.8604\n",
      "Epoch 50/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3308 - accuracy: 0.8592 - val_loss: 0.3296 - val_accuracy: 0.8606\n",
      "Epoch 51/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3304 - accuracy: 0.8611 - val_loss: 0.3244 - val_accuracy: 0.8614\n",
      "Epoch 52/100\n",
      "2602/2602 [==============================] - 120s 46ms/step - loss: 0.3288 - accuracy: 0.8599 - val_loss: 0.3209 - val_accuracy: 0.8633\n",
      "Epoch 53/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3293 - accuracy: 0.8602 - val_loss: 0.3205 - val_accuracy: 0.8641\n",
      "Epoch 54/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3290 - accuracy: 0.8603 - val_loss: 0.3195 - val_accuracy: 0.8642\n",
      "Epoch 55/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3297 - accuracy: 0.8602 - val_loss: 0.3215 - val_accuracy: 0.8635\n",
      "Epoch 56/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3286 - accuracy: 0.8609 - val_loss: 0.3177 - val_accuracy: 0.8649\n",
      "Epoch 57/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3275 - accuracy: 0.8609 - val_loss: 0.3212 - val_accuracy: 0.8645\n",
      "Epoch 58/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3248 - accuracy: 0.8614 - val_loss: 0.3177 - val_accuracy: 0.8639\n",
      "Epoch 59/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3243 - accuracy: 0.8620 - val_loss: 0.3333 - val_accuracy: 0.8598\n",
      "Epoch 60/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3223 - accuracy: 0.8640 - val_loss: 0.3158 - val_accuracy: 0.8653\n",
      "Epoch 61/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3243 - accuracy: 0.8615 - val_loss: 0.3207 - val_accuracy: 0.8638\n",
      "Epoch 62/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3175 - accuracy: 0.8657 - val_loss: 0.3280 - val_accuracy: 0.8627\n",
      "Epoch 63/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3244 - accuracy: 0.8632 - val_loss: 0.3172 - val_accuracy: 0.8660\n",
      "Epoch 64/100\n",
      "2602/2602 [==============================] - 120s 46ms/step - loss: 0.3241 - accuracy: 0.8621 - val_loss: 0.3258 - val_accuracy: 0.8620\n",
      "Epoch 65/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3219 - accuracy: 0.8644 - val_loss: 0.3208 - val_accuracy: 0.8646\n",
      "Epoch 66/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3198 - accuracy: 0.8648 - val_loss: 0.3156 - val_accuracy: 0.8662\n",
      "Epoch 67/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3229 - accuracy: 0.8637 - val_loss: 0.3213 - val_accuracy: 0.8624\n",
      "Epoch 68/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3208 - accuracy: 0.8639 - val_loss: 0.3139 - val_accuracy: 0.8672\n",
      "Epoch 69/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3180 - accuracy: 0.8651 - val_loss: 0.3166 - val_accuracy: 0.8657\n",
      "Epoch 70/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3182 - accuracy: 0.8658 - val_loss: 0.3116 - val_accuracy: 0.8685\n",
      "Epoch 71/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3178 - accuracy: 0.8651 - val_loss: 0.3221 - val_accuracy: 0.8625\n",
      "Epoch 72/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3138 - accuracy: 0.8668 - val_loss: 0.3133 - val_accuracy: 0.8682\n",
      "Epoch 73/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3185 - accuracy: 0.8657 - val_loss: 0.3441 - val_accuracy: 0.8548\n",
      "Epoch 74/100\n",
      "2602/2602 [==============================] - 120s 46ms/step - loss: 0.3152 - accuracy: 0.8667 - val_loss: 0.3118 - val_accuracy: 0.8671\n",
      "Epoch 75/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3138 - accuracy: 0.8677 - val_loss: 0.3147 - val_accuracy: 0.8674\n",
      "Epoch 76/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3127 - accuracy: 0.8679 - val_loss: 0.3124 - val_accuracy: 0.8669\n",
      "Epoch 77/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3172 - accuracy: 0.8666 - val_loss: 0.3191 - val_accuracy: 0.8672\n",
      "Epoch 78/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3134 - accuracy: 0.8681 - val_loss: 0.3097 - val_accuracy: 0.8688\n",
      "Epoch 79/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3127 - accuracy: 0.8687 - val_loss: 0.3096 - val_accuracy: 0.8690\n",
      "Epoch 80/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3117 - accuracy: 0.8681 - val_loss: 0.3109 - val_accuracy: 0.8687\n",
      "Epoch 81/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3158 - accuracy: 0.8662 - val_loss: 0.3144 - val_accuracy: 0.8694\n",
      "Epoch 82/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3135 - accuracy: 0.8679 - val_loss: 0.3114 - val_accuracy: 0.8690\n",
      "Epoch 83/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3115 - accuracy: 0.8703 - val_loss: 0.3425 - val_accuracy: 0.8566\n",
      "Epoch 84/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3101 - accuracy: 0.8691 - val_loss: 0.3056 - val_accuracy: 0.8707\n",
      "Epoch 85/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3130 - accuracy: 0.8683 - val_loss: 0.3114 - val_accuracy: 0.8698\n",
      "Epoch 86/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3107 - accuracy: 0.8691 - val_loss: 0.3094 - val_accuracy: 0.8688\n",
      "Epoch 87/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3101 - accuracy: 0.8684 - val_loss: 0.3279 - val_accuracy: 0.8615\n",
      "Epoch 88/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3119 - accuracy: 0.8690 - val_loss: 0.3333 - val_accuracy: 0.8572\n",
      "Epoch 89/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3061 - accuracy: 0.8709 - val_loss: 0.3107 - val_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3083 - accuracy: 0.8704 - val_loss: 0.3123 - val_accuracy: 0.8682\n",
      "Epoch 91/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3083 - accuracy: 0.8698 - val_loss: 0.3259 - val_accuracy: 0.8658\n",
      "Epoch 92/100\n",
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3079 - accuracy: 0.8705 - val_loss: 0.3327 - val_accuracy: 0.8602\n",
      "Epoch 93/100\n",
      "2602/2602 [==============================] - 119s 46ms/step - loss: 0.3070 - accuracy: 0.8708 - val_loss: 0.3243 - val_accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3033 - accuracy: 0.8739 - val_loss: 0.3661 - val_accuracy: 0.8424\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602/2602 [==============================] - 118s 46ms/step - loss: 0.3051 - accuracy: 0.8712 - val_loss: 0.3194 - val_accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3071 - accuracy: 0.8697 - val_loss: 0.3089 - val_accuracy: 0.8696\n",
      "Epoch 97/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3088 - accuracy: 0.8699 - val_loss: 0.3070 - val_accuracy: 0.8704\n",
      "Epoch 98/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3055 - accuracy: 0.8719 - val_loss: 0.3275 - val_accuracy: 0.8616\n",
      "Epoch 99/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3040 - accuracy: 0.8728 - val_loss: 0.3071 - val_accuracy: 0.8720\n",
      "Epoch 100/100\n",
      "2602/2602 [==============================] - 118s 45ms/step - loss: 0.3014 - accuracy: 0.8733 - val_loss: 0.3075 - val_accuracy: 0.8713\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "histories2 = []\n",
    "\n",
    "########################################################################\n",
    "\n",
    "name=\"adv-cnn1-1\"\n",
    "model4 = keras.Sequential(\n",
    "    [\n",
    "        ################# to be replaced ###############################\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        \n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        #################################################################\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(1, activation='sigmoid')   \n",
    "    \n",
    "    ],name=name\n",
    ")\n",
    "\n",
    "#model4.summary()\n",
    "\n",
    "model4.compile(loss=\"binary_crossentropy\", optimizer=SGD(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "\n",
    "histories2.append(model4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val,y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "name=\"adv-cnn1-2\"\n",
    "model5 = keras.Sequential(\n",
    "    [\n",
    "        ################# to be replaced ###############################\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        \n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        #################################################################\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(1, activation='sigmoid')   \n",
    "    \n",
    "    ],name=name\n",
    ")\n",
    "\n",
    "#model5.summary()\n",
    "\n",
    "model5.compile(loss=\"binary_crossentropy\", optimizer=SGD(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "\n",
    "histories2.append(model5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val,y_val)))\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "########################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'][1:])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'test_acc', 'train_loss', 'test_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnseminar",
   "language": "python",
   "name": "nnseminar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
